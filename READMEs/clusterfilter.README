#
# $Id$
#

The "clusterfilter" receives the files like another filter but,
instead of processing the files, it sends them to other computers
for processing.

Terminology:

  1) master - The computer that runs the clusterfilter.
  2) nodes -  The computers that will run the usual Nbsp filters,
              as configured in the master.

Requirements:

  1) The master must (NFS) export the spool directory (it can be rdonly).
     Each node must mount the master spool directory.

  2) The Nbsp data directories must be shared between the master
     and the nodes, and there are two options:

     A) The master exports the Nbsp data directory (rw), and each node
     mounts it.
     This is probably the best option but it requires a good central
     data server.

     B) Each node exports the apropriate subdirectory of the data
     directory and the master mounts all of them. This is economical
     and good in terms of performance because the nodes write the processed
     data to their local disks, and the master pulls from them what
     it needs when it needs.

  3) The noaaport user in the master must be able to login via ssh without
     a password in each node. In practice this means creating the ssh key
     in the master and copying it to the .ssh directory of the noaaport user
     in each node

         /var/noaaport/.ssh

Configuration:

  In the master

      set features(clusterfilter) 1

  in the features.conf file, and the appropriate entries in
  "clusterfilter.conf" as explained below. In each node, the
  filters that it will execute must be configured as usual,
  but no further configuration is required.

Configuration of clusterfilter.conf:

  The best is to consider an example of two nodes, each executing
a couple of filters.
  
#
# Two working nodes, "diablo" and "caribe"
#
set clusterfilter(nodes) [list "caribe" "diablo"];

#
# caribe
#
set clusterfilter(enable,caribe,gpfilter) 2;   # enable work-crew mode
set clusterfilter(enable,caribe,gisfilter) 1;

# Possible ssh options
#
# set clusterfilter(sshopts,caribe) "-p 2222";  # if ssh lsitens in another port

# What to send to this filter (see more examples below). If nothing is set
# then the default is equivalent to the first option (all)
#
# set clusterfilter(fname_uregex,caribe) ".*";       # all
# set clusterfilter(fname_uregex,caribe) "_sdus";    # radar
# set clusterfilter(fname_uregex,caribe) "-n(0|1|2|3)(r|s|v)jua"; # jua radar

#
# diablo
#
set clusterfilter(enable,diablo,dafilter) 2;   # work-crew mode
set clusterfilter(enable,diablo,rstfilter) 1;
set clusterfilter(fname_uregex,diablo) {_tig}; # all sat image data files

Configuration of clusterfilter-schedule.conf

  If the Nbsp data directories are setup following option (2A) above
then nothing else is required. But if option (2B) is followed,
then the clusterfilter scheduler must be enable to execute
the hourly cleanup script in each node in order to maintain the
data directories of the nodes.

  First, in "clusterfilter.conf" (or prerefably in the "site" copy
of this file "site/clusterfilter.conf")

   set clusterfilter(scheduler_enable) 1

must be set. Then the file "clusterfilter-schedule.rc" must be created
with an entry for each node. A sample file provided in
the Nbsp confguration directory, and it only requires that the
setting

set nodelist [list]

be edited to include the appropriate list of nodes. As usual a better strategy
is to copy this file to the "site" subdirectory, and in this way the local
settings will not be overwitten during an upgrade.
